# event-scheduler

The **event scheduler** is an interface exposed in web synth for the purpose of scheduling events to take place in the future and synchronizing time/beats between all parts of the application.  Under the hood, it manages a global rhythm counter that runs on the audio thread based on the high-granularity audio context clock.  It maintains a global beat counter with a dynamically adjustable BPM that allows for all modules to be synchronized to the same rhythm.

It exposes this functionality to the main/render thread via event registration, similar to that of `setTimeout` but based off of this global synchronized clock.  Events can be schedule in terms of both beats as well as time, and it takes care of dealing with dynamic BPM changes under the hood.  It is currently used by the [[midi-editor]] and [[sequencer]] to support playback.

## drawbacks / downsides

There are some issues with this interface that can create problems for highly-accurate audio rendering and playback needs.  The main issue is that web audio is only accessible from the main thread; you can't dynamically modify the [[audio-graph]] from the audio rendering thread; the only thing you can do is emit outputs via `AudioWorkletProcessor`s.  The only way to communicate data to the main thread from the web audio rendering thread is by using the message ports, and those are by design asynchronous communication channels that impose some inescapable latency to scheduled events.

_note to self that I just thought of: perhaps we can measure this latency at application startup and automatically adjust for it in the event scheduler.  That sounds cool - we just send our events a bit earlier according to the measured latency.  This won't help with jitter or contention, but it will help in the base case.  cool idea._

Additionally, since event callbacks are handled in the main thread, they can suffer from contention for CPU resources from the UI and other things running in the application.  If the application is busy rendering a complicated visualization or performing some heavy React rendering, it's possible for audio playback to get glitchy or events to lag.  This was originally quite bad, but switching to React concurrent mode helped greatly since it avoids the length of blocking periods of the event loop and reduces the average latency between when events are received over the message port and when they are actually processed on the event loop.

## alternatives

In the future, is sample-accurate audio rendering and playback is something that we want to achieve, all of the logic for things like MIDI events, sample playback, and other dynamic use-cases will need to be handled entirely on the audio thread in AWPs rather than bridged between the main thread and the audio rendering thread.  Doing that supports synchronizing execution between all AWPs sample-per-sample with no latency or overhead.  However, this introduces a ton of complexity and makes things a lot more clunky to implement, so the event schedule is used widely due to how generic it is (scheduled events are arbitrary JS callbacks on the main thread) and how simple its interface is.

[//begin]: # "Autogenerated link references for markdown compatibility"
[midi-editor]: midi-editor "midi-editor"
[sequencer]: sequencer "sequencer"
[audio-graph]: audio-graph "audio graph"
[//end]: # "Autogenerated link references"